{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "municipal-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders          import *\n",
    "from sklearn.base               import BaseEstimator\n",
    "from sklearn.compose            import *\n",
    "from sklearn.decomposition      import PCA\n",
    "from sklearn.ensemble           import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute             import *\n",
    "from sklearn.linear_model       import LogisticRegression\n",
    "from sklearn.metrics            import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection    import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline           import Pipeline\n",
    "from sklearn.preprocessing      import StandardScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "colonial-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to remove warnings for RandomizedSearchCV so the results are easier to read.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "turkish-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into pandas dataframe.\n",
    "combine_df = pd.read_csv('red_and_white_wine.csv')\n",
    "\n",
    "# Target Transformation: treating \"great\" wine as wine that has a quality score greater than or equal to 7. All else will be treated as \"poor\" quality\n",
    "combine_df['quality'] = combine_df.apply(lambda row: 1 if row['quality'] >= 7 else 0, axis = 1)\n",
    "\n",
    "# Seperating y and X\n",
    "y = combine_df['quality']\n",
    "X = combine_df[['fixed acidity', \n",
    "                'volatile acidity', \n",
    "                'citric acid', \n",
    "                'residual sugar',\n",
    "                'chlorides', \n",
    "                'free sulfur dioxide', \n",
    "                'total sulfur dioxide', \n",
    "                'density',\n",
    "                'pH', \n",
    "                'sulphates', \n",
    "                'alcohol', \n",
    "                'red wine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "consolidated-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to use for RandomizedSearchCV to tune hyperparameters\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uniform-vinyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed: 11.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 metric: 88.48%\n",
      "\n",
      "Confusion Matrix:\n",
      "      0    1\n",
      "0  1030   18\n",
      "1   118  134\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      1048\n",
      "           1       0.88      0.53      0.66       252\n",
      "\n",
      "    accuracy                           0.90      1300\n",
      "   macro avg       0.89      0.76      0.80      1300\n",
      "weighted avg       0.89      0.90      0.88      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed\n",
    "np.random.seed(21)\n",
    "\n",
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "\n",
    "# Splitting the dataset into training (80%) and testing (20%)\n",
    "X_training, X_test, y_training, y_test = train_test_split(X, \n",
    "                                                          y, \n",
    "                                                          test_size=.2,\n",
    "                                                          random_state=random_state)\n",
    "\n",
    "# Splitting the training dataset into a train set (80%) and a validation set (20%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_training, \n",
    "                                                      y_training, \n",
    "                                                      test_size=.2,\n",
    "                                                      random_state=random_state)\n",
    "\n",
    "\n",
    "# Storing the categorical variable (whether it's red wine or white wine).\n",
    "cat_cols = ['red wine']\n",
    "\n",
    "# Storing the continuous variables.\n",
    "cot_cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "            'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "            'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "# No missing data in dataset, adding imputers for missing test data to build a more robust system. \n",
    "\n",
    "# Using mode for this imputer it makes the most sense for categorical variables.\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "                    ])\n",
    "\n",
    "# Using median for this imputer as it makes the most sense for continuous variables.\n",
    "cont_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                      ('imputer', SimpleImputer(strategy='median', add_indicator=True))\n",
    "                     ])\n",
    "\n",
    "# ColumnTransformer for column-by-column preprocessing.\n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, cat_cols),\n",
    "                                   ('continuous', cont_pipe, cot_cols)\n",
    "                                  ])\n",
    "\n",
    "# Preparing pipeline for RandomizedSearchCV.\n",
    "pipe = Pipeline([('prep', preprocessing),\n",
    "                 # Principal Component Analysis.\n",
    "                 ('pca', PCA()),\n",
    "                 # Dummy placeholder so we can test different algorithms with different hyperparameters.\n",
    "                 ('clf', DummyEstimator())])\n",
    "\n",
    "# Hyperparameter search across features, algorithms, and hyperparameters\n",
    "search_space = [\n",
    "                # Seeing if dimension reduction is useful for our model.\n",
    "                {'pca__n_components': [0,1,2,3,4,5]},\n",
    "                \n",
    "                {'clf': [LogisticRegression()],\n",
    "                 # Searching through penalty as it is used to specify the norm used in penalization. Some penalties work better for certain datasets.\n",
    "                 'clf__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                 # C is the inverse of regularizaiton strength. Has an effect on the lambda regulator.\n",
    "                 'clf__C': np.logspace(0, 4, 10),\n",
    "                 # The weights associated with the classes. Can help with imbalanced datasets.\n",
    "                 'clf__class_weight': ['balanced','None'],\n",
    "                 # Determines which algorithm is used for optimization. Different algorithms are useful for different datasets.\n",
    "                 'clf__solver' : ['newton-cg','lbfgs', 'liblinear', 'sag', 'saga']},\n",
    "    \n",
    "                {'clf': [RandomForestClassifier()],\n",
    "                 # The function to measure the quality of a split for each decision node. Can help with computation speed and accuracy.\n",
    "                 'clf__criterion': ['gini', 'entropy'],\n",
    "                 # The number of trees used in the forest. Can help with generalizing the model.\n",
    "                 'clf__n_estimators': [50,100,150,200],\n",
    "                 # The number of features to consider for each split. Can help with generalization.\n",
    "                 'clf__max_features': ['auto','sqrt','log2'],\n",
    "                 # The maximum depth of a tree. Can also help with generalization.\n",
    "                 'clf__max_depth': [2,3,4,5,6,7,8,9,10,20,50,100],\n",
    "                 # The weights associated with the classes. Can help with imbalanced datasets.\n",
    "                 'clf__class_weight' : [None, 'balanced','balanced_subsample']},\n",
    "                \n",
    "                {'clf': [ExtraTreesClassifier()],\n",
    "                 # The function to measure the quality of a split for each decision node. Can help with computation speed and accuracy.\n",
    "                 'clf__criterion': ['gini', 'entropy'],\n",
    "                 # The number of trees used in the forest. Can help with generalizing the model.\n",
    "                 'clf__n_estimators': [50,100,150,200],\n",
    "                 # The number of features to consider for each split. Can help with generalization.\n",
    "                 'clf__max_features': ['auto','sqrt','log2'],\n",
    "                 # The maximum depth of a tree. Can also help with generalization.\n",
    "                 'clf__max_depth': [2,3,4,5,6,7,8,9,10,20,50,100],\n",
    "                 # The weights associated with the classes. Can help with imbalanced datasets.\n",
    "                 'clf__class_weight' : [None, 'balanced','balanced_subsample']}]\n",
    "\n",
    "# Using Randomized Search as it can often lead to better results in a quicker time frame than grid search.\n",
    "clf_algos_rand = RandomizedSearchCV(random_state=random_state,\n",
    "                                    estimator=pipe, \n",
    "                                    param_distributions=search_space,\n",
    "                                    # Using 300 candidates for our model search.\n",
    "                                    n_iter=200,\n",
    "                                    # Using StratifiedKFold to help with data imbalance.\n",
    "                                    cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state),\n",
    "                                    # Using n_jobs as one as it helps with the reproducibility of my results.\n",
    "                                    n_jobs=1,\n",
    "                                    verbose=1)\n",
    "\n",
    "best_model = clf_algos_rand.fit(X_train, y_train)\n",
    "\n",
    "# Plugging in the best model from the RandomizedSearchCV\n",
    "pipe = Pipeline([('prep', preprocessing),\n",
    "                 ('rf',  best_model.best_estimator_.get_params()['clf'])])\n",
    "\n",
    "pipe.fit(X_training, y_training)\n",
    "y_pred = pipe.predict(X_test)\n",
    "f1_test  = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f\"F1 metric: {f1_test:.2%}\\n\")\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(pd.DataFrame(confusion_matrix(y_true=y_test, y_pred=y_pred)))\n",
    "\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "knowing-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA()\n"
     ]
    }
   ],
   "source": [
    "# PCA was not used in our final model pipeline\n",
    "print(best_model.best_estimator_.get_params()['pca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "digital-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(class_weight='balanced', criterion='entropy',\n",
      "                     max_depth=100, max_features='log2', n_estimators=50)\n"
     ]
    }
   ],
   "source": [
    "# Visually displayed single, best final model and final model parameters\n",
    "print(best_model.best_estimator_.get_params()['clf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-cyprus",
   "metadata": {},
   "source": [
    "## Chosen Model:\n",
    "\n",
    "From my experience the three best models I have used in classification were Logistic Regression, Random Forest Classifier, Extra Trees Classifier. I chose these three models as in the past, they have performed extremely well on binary classification problems. I used a pipeline to search through hyperparameters for each model, and Extra Trees Classifier was chosen to be the best possible model of the three. Extra Trees Classifier fits a number of randomized decision trees on various subsamples of the datatset.\n",
    "\n",
    "## Hyperparameters for best model:\n",
    "\n",
    "The best model had the following hyperparameters: class_weight='balanced', criterion='entropy', max_depth=100, max_features='log2', n_estimators=50. \n",
    "\n",
    "## Comments on hyperparameters:\n",
    "\n",
    "The class_weight hyperparameter refers to the weights associated with each class. In this case, \"balanced\" means the weights on each class are automatically adjusted to be inversely proportional to class frequencies based on the values of the target variable.\n",
    "\n",
    "The critierion hyperparameter 'entropy' is important as the critierion is used to measure the quality of the split. \"Entropy\" means that our best model is calculating the quality of a split based off of the entropy score instead of the Gini value. Entropy is a more complex computation and sometimes gives results that are slightly better while Gini is computationally less expensive.\n",
    "\n",
    "The max_depth hyperparameter is important as it indicates the maximum depth that an individual tree can reach. For ExtraTreesClassfier, max depth restricts how many splits a individual tree can make, keeping it from getting too specific to the training set, thus possibly increasing the model's generality.\n",
    "\n",
    "The max_features hyperparameter designates the number of features that will be used while looking for the best split for each node. For our model, the best model uses the log base 2 of the total number of features to pick a subsample of features that will be chosen from at a specific decision node. Restricting the available features when searching for a split can help improve the generality of the model.\n",
    "\n",
    "The n_estimators hyperparameter designates the number of trees in our ExtraTreesClassifier. The number of trees can increase the accuracy greatly (initially). However, at a specific point the increase of accuracy with the addition of one tree decreases greatly, thus it is important to find a balance. In this case, it seems the n_estimators = 50 seems to work as a good balance for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 metric: 88.48%\n",
    "\n",
    "Confusion Matrix:\n",
    "      0    1\n",
    "0  1030   18\n",
    "1   118  134\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.98      0.94      1048\n",
    "           1       0.88      0.53      0.66       252\n",
    "\n",
    "    accuracy                           0.90      1300\n",
    "   macro avg       0.89      0.76      0.80      1300\n",
    "weighted avg       0.89      0.90      0.88      1300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-witness",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "\n",
    "For this model I decided to use two evaluation metrics: the F1 score, the confusion matrix, and the classification report. I used the F1 score as it gives insight on the classification problems a model may be facing. The F1 score equally weighs precision and recall for our model. The F1 score for our model is high (88.48%), thus it has a nice balance between precision and recall.\n",
    "\n",
    "However, to get a clearer picture, I decided to include the confusion matrix and the classification report. From the confusion matrix two things stand out, our False Negative rate for \"great\" wines is high (118) when compared to our True Positive rate (134), which indicates that our model was having a hard time predicting if a wine had a quaity score of 7 or above. The second thing that stands out is that our model has a great False Negative rate, only predicting a few (18) low quality wines as higher quality. From the classification report we see that the precision and recall score for low quality wines (wines with quality scores below 7) was very high (precision=0.90, recall=0.98), while the precision and recall scores for high quality wine (wines with quality scores 7 and above) was lower (precision=0.88, recall=0.53), especially the recall score.\n",
    "\n",
    "All in all, our model performed well and had a good balance between the recall and precision score. Our metrics tell us that our model is relatively good on predicting the quality of wine, especially predicting if wine is of poor quality. However, there is still some room for improvement in predicting \"great\" wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pending-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('prep',\n",
      "                 ColumnTransformer(transformers=[('categorical',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(add_indicator=True,\n",
      "                                                                                 strategy='most_frequent')),\n",
      "                                                                  ('ohe',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  ['red wine']),\n",
      "                                                 ('continuous',\n",
      "                                                  Pipeline(steps=[('scaler',\n",
      "                                                                   StandardScaler()),\n",
      "                                                                  ('imputer',\n",
      "                                                                   SimpleImputer(add_indicator=True,\n",
      "                                                                                 strategy='median'))]),\n",
      "                                                  ['fixed acidity',\n",
      "                                                   'volatile acidity',\n",
      "                                                   'citric acid',\n",
      "                                                   'residual sugar',\n",
      "                                                   'chlorides',\n",
      "                                                   'free sulfur dioxide',\n",
      "                                                   'total sulfur dioxide',\n",
      "                                                   'density', 'pH', 'sulphates',\n",
      "                                                   'alcohol'])])),\n",
      "                ('rf',\n",
      "                 ExtraTreesClassifier(class_weight='balanced',\n",
      "                                      criterion='entropy', max_depth=100,\n",
      "                                      max_features='log2', n_estimators=50))])\n"
     ]
    }
   ],
   "source": [
    "# Display all pipelines steps and all non-default hyperparameters\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-consent",
   "metadata": {},
   "source": [
    "## Summary of findings\n",
    "\n",
    "I believe that I came up with a decent model to predict the quality of red/white wine based off its chemical composition. I think that while there is some room for improvement, especially in the case of having better precision and recall for \"great\" wines, this model is a great start and does a decent job as can be seen from the F1 score and the confusion matrix/classification report. I think things worked well because I chose three models that work well for this kind of classification problem, and I think the target engineering that I performed on the \"quality\" variable was extremely beneficial. I believe that it was smart to switch this into a binary prediction problem, and that the distinction between \"poor\" wine and \"great\" wine is accurate, as most people would state that anything below 7/10 should be seen as average or worse than average.\n",
    "\n",
    "This project matters because the wine industry has expanded exponentially over the past two decades. As more people become interested in wine, the importance of identifying good wine increases and become more lucrative.\n",
    "\n",
    "My next steps would be to try different more powerful algorithms. I would want to try a XGBoost model and compare it against my own, to see if there is any significant change in the recall and precision score for \"great\" wines. I may also want to include more data from other wine raters, as a diverse range of raters would help make my model more robust for the business enviroment as currently we only have ratings from the original data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
